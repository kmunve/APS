{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREE\n",
    "\n",
    "- is another method we can use for finding relationship between target and one or more predictors\n",
    "- decision trees can be used for both categorical and continous targets (so both classification and regression tasks -  today we'll be focusing on regression trees)\n",
    "- the main idea is to create a tree of decisions that best partitions the data \n",
    "- creating a tree involves deciding on which features to split the data and what conditions to use for splitting, as well as with knowing when to stop\n",
    "\n",
    "\n",
    "\n",
    "![title](pictures/decision_tree.png)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "pd.set_option(\"display.max_rows\",6)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('varsom_ml_preproc.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# RANDOM FORESTS\n",
    "\n",
    "- are using Bagging (bootstrap aggregating) algorithm\n",
    "\n",
    "- **bagging** is an ensemble learning where we build each model using the same algorithm, but we train each learner on different set of data\n",
    "\n",
    "\n",
    "# GRADIENT BOOSTED TREES\n",
    "\n",
    "- are using Boosting (Ada Boost) algorithm\n",
    "\n",
    "- **boosting** is a variation of ensemble trees that strives to improve the learners by focusing on areas where the system is not performing well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can start by creating one decision tree of depth 3 using all features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dec_tree = DecisionTreeRegressor(random_state=222, max_depth = 3)\n",
    "\n",
    "dec_tree.fit(X_train, y_train) # we're using the same data as in last linear model\n",
    "\n",
    "predictions_dt = dec_tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to visualize the tree we need package graphviz and it needs to be installed \n",
    "\n",
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} python-graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tree\n",
    "\n",
    "from sklearn import tree\n",
    "import graphviz \n",
    "\n",
    "dot_data = tree.export_graphviz(dec_tree, out_file=None, \n",
    "                         feature_names=boston_data.drop('MEDV', axis=1).columns,   \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "\n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can see that first we're splitting on RM and LSTAT, meaning that those are the most important variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to evaluate our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RSS for decision tree model is {0:.2f}\".format(RSS(y_test, predictions_dt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Decision tree R^2: %.4f' % dec_tree.score(X_test, y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the linear model is performing better than decision tree model. \n",
    "\n",
    "But we have selected tree of depth 3 - could we get a better model by selecting different depth? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RSS_new(f, y, X):\n",
    "    return sum((y - f.predict(X))**2)\n",
    "\n",
    "depths = range(1, 10)\n",
    "\n",
    "tree_models = [DecisionTreeRegressor(random_state=222, max_depth=d).fit(X_train, y_train) for d in depths]\n",
    "tree_RSS = [RSS_new(f, y_test, X_test) for f in tree_models]\n",
    "\n",
    "\n",
    "plt.plot(depths, tree_RSS, color = 'red')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.ylabel('RSS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so let's create a tree with depth = 6\n",
    "\n",
    "dec_tree = DecisionTreeRegressor(random_state=222, max_depth = 6)\n",
    "\n",
    "dec_tree.fit(X_train, y_train) # we're using the same data as in last linear model\n",
    "\n",
    "predictions_dt = dec_tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tree\n",
    "\n",
    "from sklearn import tree\n",
    "import graphviz \n",
    "\n",
    "dot_data = tree.export_graphviz(dec_tree, out_file=None, \n",
    "                         feature_names=boston_data.drop('MEDV', axis=1).columns,   \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "\n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RSS for decision tree model is {0:.2f}\".format(RSS(y_test, predictions_dt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Decision tree R^2: %.4f' % dec_tree.score(X_test, y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see slight improvement in both RSS and $R^{2}$ - but does such a small improvement justifies usage of much more complex model? \n",
    "\n",
    "\n",
    "Maybe we'l get better results with **random forests** and/or **gradient boosted trees**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state = 422, max_depth = 6)\n",
    "forest_reg.fit(X_train, y_train)\n",
    "\n",
    "predictions_rf = forest_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RSS for random forest model is {0:.2f}\".format(RSS(y_test, predictions_rf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random forest R^2: %.4f' % forest_reg.score(X_test, y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a gradient boosting regressor with random state 222 and max depth 6 \n",
    "\n",
    "# Print RSS and R^2 for your model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "grad_boost = GradientBoostingRegressor(random_state = 222, max_depth = 6)\n",
    "\n",
    "grad_boost.fit(X_train, y_train)\n",
    "\n",
    "predictions_gb = grad_boost.predict(X_test)\n",
    "\n",
    "print(\"RSS for gradient boosted tree model is {0:.2f}\".format(RSS(y_test, predictions_gb)))\n",
    "print()\n",
    "print('Gradient boosted tree R^2: %.4f' % grad_boost.score(X_test, y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, very small difference between random forests and boosted trees.\n",
    "\n",
    "We can use random forest model.\n",
    "\n",
    "Last thing we can check is imprtance of variables - if some of the feature are not as useful as the other in explaining the variability in our target variable, we cn exclude them in order to simplify our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_labels = np.array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'RM', 'AGE', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'])\n",
    "importance = forest_reg.feature_importances_\n",
    "feature_indexes_by_importance = importance.argsort()\n",
    "for index in feature_indexes_by_importance:\n",
    "    print('{}-{:.2f}%'.format(feature_labels[index], (importance[index] *100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluding the variables where impotance is less than 1%\n",
    "\n",
    "X = boston_data[['CRIM', 'RM', 'TAX', 'PTRATIO', 'LSTAT', 'AGE', 'B', 'INDUS']]\n",
    "y = boston_data[\"MEDV\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_boost = GradientBoostingRegressor(random_state = 222, max_depth = 6)\n",
    "\n",
    "grad_boost.fit(X_train, y_train)\n",
    "\n",
    "predictions_gb = grad_boost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RSS for gradient boosted tree model is {0:.2f}\".format(RSS(y_test, predictions_gb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gradient boosted tree R^2: %.4f' % grad_boost.score(X_test, y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
