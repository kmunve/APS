{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training dataset from varsom data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from flatten_json import flatten\n",
    "from pprint import pprint\n",
    "# make sure the aps module is in the pythonpath\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "APS_dir = str(Path.cwd().parents[1])\n",
    "if APS_dir not in sys.path:\n",
    "    sys.path.append(APS_dir)\n",
    "import aps.aps_io.get_forecasts as gf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api01.nve.no/hydrology/forecast/avalanche/v4.0.0/api/AvalancheWarningByRegion/Detail/3046/2/2017-12-01/2018-05-31 \n",
      "\n",
      " <class 'list'> 8372\n"
     ]
    }
   ],
   "source": [
    "# Nordvestlandet: Trollheimen, Romsdal, Sunnmøre\n",
    "#warnings, url = gf.get_warnings_as_json([3022, 3023, 3024], \"2017-12-01\", \"2018-05-31\", lang_key=2, simple=False, recursive_count=5)\n",
    "\n",
    "# Østlandet: Jotunheimen, Hallingdal, Vest-Telemark\n",
    "#warnings, url = gf.get_warnings_as_json([3028, 3032, 3035], \"2017-12-01\", \"2018-05-31\", lang_key=2, simple=False, recursive_count=5)\n",
    "\n",
    "# Hele Norge\n",
    "reg_ids_ = list(np.arange(3001, 3047))\n",
    "warnings, url = gf.get_warnings_as_json(reg_ids_, \"2017-12-01\", \"2018-05-31\", lang_key=2, simple=False, recursive_count=5)\n",
    "\n",
    "### Use this small data extraction for testing\n",
    "#warnings, url = gf.get_warnings_as_json([3022], \"2018-01-01\", \"2018-01-15\", lang_key=2, simple=False, recursive_count=5)\n",
    "\n",
    "print(url, '\\n\\n', type(warnings), len(warnings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since get_warnings_as_json returns a list, we have to apply \"flatten\" to each item... \n",
    "warnings_flattened = (flatten(w, root_keys_to_ignore={'CountyList', 'MunicipalityList'}) for w in warnings)\n",
    "# TODO: avalanche problems are not labeled correctly by their priority - need to correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "flatten requires a dictionary input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7468422b0e5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwarnings_flattened\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ndim'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-1b38819dffe8>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# since get_warnings_as_json returns a list, we have to apply \"flatten\" to each item...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwarnings_flattened\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroot_keys_to_ignore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'CountyList'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'MunicipalityList'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;31m# TODO: avalanche problems are not labeled correctly by their priority - need to correct.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Anaconda3\\lib\\site-packages\\flatten_json.py\u001b[0m in \u001b[0;36mflatten\u001b[0;34m(nested_dict, separator, root_keys_to_ignore)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mflattened\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \"\"\"\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnested_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"flatten requires a dictionary input\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseparator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"separator must be a string\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: flatten requires a dictionary input"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(warnings_flattened)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2 = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create necessary columns and populate them\n",
    "for index, row in df.iterrows():\n",
    "    for i in range(5):\n",
    "        for j in range(4):\n",
    "            try:\n",
    "                col_name = '{MWType} {MWSubType}'.format(MWType=row['MountainWeather_MeasurementTypes_{0}_Name'.format(i)], MWSubType=row['MountainWeather_MeasurementTypes_{0}_MeasurementSubTypes_{1}_Name'.format(i, j)])\n",
    "                col_name = col_name.replace(' ', '_')\n",
    "                if col_name in df.columns.values:\n",
    "                    df.loc[index, col_name] = row['MountainWeather_MeasurementTypes_{0}_MeasurementSubTypes_{1}_Value'.format(i, j)]\n",
    "                else:\n",
    "                    df[col_name] = np.nan\n",
    "                    print('Created column: ', col_name)\n",
    "                    df.loc[index, col_name] = row['MountainWeather_MeasurementTypes_{0}_MeasurementSubTypes_{1}_Value'.format(i, j)]\n",
    "                    \n",
    "                #print('{MWType} {MWSubType} = {MWSubTypeValue}'.format(MWType=df.loc[index]['MountainWeather_MeasurementTypes_{0}_Name'.format(i)],\n",
    "                 #                                                      MWSubType=df.loc[index]['MountainWeather_MeasurementTypes_{0}_MeasurementSubTypes_{1}_Name'.format(i, j)],\n",
    "                  #                                                     MWSubTypeValue=df.loc[index]['MountainWeather_MeasurementTypes_{0}_MeasurementSubTypes_{1}_Value'.format(i, j)]))\n",
    "            except KeyError:\n",
    "                print('MountainWeather_MeasurementTypes_{0}_MeasurementSubTypes_{1}... does not exist - moving on.'.format(i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['ValidFrom',\n",
    "    'Temperature_Max',\n",
    "    'Temperature_Min',\n",
    "    'Wind_Direction',\n",
    "    'Wind_Speed']].head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all data before filtering and splitting\n",
    "df.to_csv('varsom_all.csv', index_label='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training data\n",
    "We want to predict the main avalanche problem. Therefore we store the AP1_type and AP1_score in its own dataframe.\n",
    "We remove all parameters that are irrelevant for the algorithm, e.g. UtmZone, ValidTo..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make all columns used in the analysis numerical and adjust indicies where necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../config/snoskred_keys.json') as jdata:\n",
    "    snoskred_keys = json.load(jdata)\n",
    "\n",
    "pprint(snoskred_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['AvalancheProblems_1_AvalPropagationId'].unique(), type(df['AvalancheProblems_1_AvalPropagationId'].unique()[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change wind speeds to numerical values\n",
    "df['Wind_Speed_Num'] = df['Wind_Speed'].apply(lambda i: snoskred_keys['beaufort_scale_en'][i])\n",
    "df['Wind_Direction_Num'] = df['Wind_Direction'].apply(lambda i: 0 if i == None else snoskred_keys['wind_dir_conv_en'][i])\n",
    "\n",
    "# Re-group AvalancheProblemType\n",
    "# AvalancheProblemType grouped by PWL, wet slab, wet loose, dry loose, storm slab, and wind slab (and glide avalanche).\n",
    "df['AvalancheProblems_0_Class_AvalancheProblemTypeId'] = df['AvalancheProblems_0_AvalancheProblemTypeId'].apply(lambda i: 0 if i == np.nan else np.int(snoskred_keys['Class_AvalancheProblemTypeId'][str(int(i))]))\n",
    "df['AvalancheProblems_1_Class_AvalancheProblemTypeId'] = df['AvalancheProblems_1_AvalancheProblemTypeId'].apply(lambda i: 0 if str(i) == str(np.nan) else np.int(snoskred_keys['Class_AvalancheProblemTypeId'][str(int(i))]))\n",
    "\n",
    "# Distribution is labeled _Propagation_ in the API and has five classes. Change name to _AvalDistribution_ and merge the uper three classes into one called _widespread_.\n",
    "df['AvalancheProblems_0_Class_AvalDistributionId'] = df['AvalancheProblems_0_AvalPropagationId'].apply(lambda i: 0 if str(i) == str(np.nan) else np.int(snoskred_keys['Class_AvalDistributionId'][str(int(i))]))\n",
    "df['AvalancheProblems_1_Class_AvalDistributionId'] = df['AvalancheProblems_1_AvalPropagationId'].apply(lambda i: 0 if str(i) == str(np.nan) else np.int(snoskred_keys['Class_AvalDistributionId'][str(int(i))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only columns that hold numerical values.\n",
    "# AvalCause, AvalancheExt and AvalancheProblemType are directly correlated - keep only re-grouped ..._Class_AvalancheProblemType.\n",
    "df_numdata = df.filter(['AvalancheProblems_0_AvalProbabilityId',\n",
    "                        'AvalancheProblems_0_Class_AvalDistributionId',\n",
    "                        'AvalancheProblems_0_AvalTriggerSimpleId',\n",
    "                        'AvalancheProblems_0_AvalancheProblemId',\n",
    "                        'AvalancheProblems_0_Class_AvalancheProblemTypeId',\n",
    "                        'AvalancheProblems_0_AvalancheTypeId',\n",
    "                        'AvalancheProblems_0_DestructiveSizeExtId',\n",
    "                        'AvalancheProblems_1_AvalProbabilityId',\n",
    "                        'AvalancheProblems_1_Class_AvalDistributionId',\n",
    "                        'AvalancheProblems_1_AvalTriggerSimpleId',\n",
    "                        'AvalancheProblems_1_AvalancheProblemId',\n",
    "                        'AvalancheProblems_1_Class_AvalancheProblemTypeId',\n",
    "                        'AvalancheProblems_1_AvalancheTypeId',\n",
    "                        'AvalancheProblems_1_DestructiveSizeExtId',\n",
    "                        'DangerLevel',\n",
    "                        'ValidFrom',\n",
    "                        'Rainfall_Most_exposed_area',\n",
    "                        'Rainfall_Average',\n",
    "                        'Wind_Speed_Num',\n",
    "                        'Wind_Direction_Num',\n",
    "                        'Temperature_Min',\n",
    "                        'Temperature_Max',\n",
    "                        'Temperature_masl',\n",
    "                        'Freezing_Level_masl'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numdata.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are no weired values.\n",
    "for col in df_numdata.drop(['ValidFrom'], axis=1).columns.values:\n",
    "    print(col, ': ', df_numdata[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all columns without data\n",
    "#df_numdata = df_numdata.dropna(axis=1)\n",
    "\n",
    "df_numdata.to_csv('varsom_numdata.csv', index_label='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into a training- and test-dataset\n",
    "Randomly choose indicies that should serve a test data and which are removed from the training data.\n",
    "Maybe write a function that chooses a certain % as test dxata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly shuffle the index of nba.\n",
    "random_indices = np.random.permutation(df_numdata.index)\n",
    "# Set a cutoff for how many items we want in the test set (in this case 1/3 of the items)\n",
    "test_cutoff = np.int(np.floor(len(df_numdata)/3))\n",
    "print(test_cutoff)\n",
    "# Generate the test set by taking the first 1/3 of the randomly shuffled indices.\n",
    "df_test = df_numdata.loc[random_indices[1:test_cutoff]]\n",
    "# Generate the train set with the rest of the data.\n",
    "df_train = df_numdata.loc[random_indices[test_cutoff:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the target variable in its own dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_data = df_train.drop(['DangerLevel'], axis=1)\n",
    "df_test_data = df_test.drop(['DangerLevel'], axis=1)\n",
    "\n",
    "df_train_target = df_train.filter(['DangerLevel'], axis=1)\n",
    "df_test_target = df_test.filter(['DangerLevel'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_data.to_csv('varsom_train_data.csv', index_label='index')\n",
    "df_test_data.to_csv('varsom_test_data.csv', index_label='index')\n",
    "\n",
    "df_train_target.to_csv('varsom_train_target.csv', index_label='index')\n",
    "df_test_target.to_csv('varsom_test_target.csv', index_label='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
